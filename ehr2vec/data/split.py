import logging
import numpy as np
from itertools import combinations
from typing import Iterator, Tuple, List

from ehr2vec.common.utils import Data
from sklearn.model_selection import KFold

logger = logging.getLogger(__name__)       


def get_n_splits_cv(data: Data, n_splits: int, indices:list=None)->Iterator[Tuple[List[int], List[int]]]:
    """
    Generate indices for n_splits cross-validation.

    Parameters:
    data (Data): Data object containing 'pids'.
    n_splits (int): Number of folds for cross-validation.
    indices (List[int], optional): List of indices to be used. If None, all indices from data are used.

    Yields:
    Iterator[Tuple[List[int], List[int]]]: Iterator over tuples of train and validation indices for each fold.
    """
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

    if indices is None:
        logger.info("Using all indices")
        indices = list(range(len(data.pids)))
    # Iterate over each fold generated by KFold
    for train_idx, val_idx in kf.split(indices):
        # Map the relative indices to actual indices in the data
        train_indices = [indices[i] for i in train_idx]
        val_indices = [indices[i] for i in val_idx]

        yield train_indices, val_indices
    
def get_n_splits_cv_k_over_n(data: Data, k:int, n:int)->Iterator[Tuple[Data,Data]]:
    """
    Splits data into k sets, with n sets used for training and the remaining sets for validation.
    
    Parameters:
    data: The dataset to be split.
    k: Total number of subsets to split the data into.
    n: Number of subsets to be used for training in each fold.
    
    Yields:
    Tuples of training and validation indices for each fold.
    """
    indices = np.arange(len(data.pids))
    split_size = len(indices) // k
    remainder = len(indices) % k
    sets = {i: indices[i * split_size:][:split_size] for i in range(k)}
    
    # Handle the remainder by adding it to the last set
    if remainder:
        sets[k - 1] = np.concatenate((sets[k - 1], indices[-remainder:]))
    # Generate all combinations of picking n subsets out of k for training
    for training_keys in combinations(sets.keys(), n):
        train_indices = np.concatenate([sets[key] for key in training_keys])
        validation_keys = [key for key in sets.keys() if key not in training_keys]
        validation_subsets = [sets[key] for key in validation_keys]

        yield train_indices, validation_subsets, validation_keys

